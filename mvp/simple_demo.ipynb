{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+--------------------+\n",
      "| _c0|_c1|                 _c2|\n",
      "+----+---+--------------------+\n",
      "|null|  0|<p> i am trying t...|\n",
      "|   0|  1|<p> i understand ...|\n",
      "|   1|  2|\"<p> i am making ...|\n",
      "|   2|  3|<p> i am trying t...|\n",
      "|   3|  4|\"<p> when i am re...|\n",
      "|   4|  5|<p> i am trying t...|\n",
      "|   5|  6|<p> wanted to get...|\n",
      "|   6|  7|\"<p> i'm wonderin...|\n",
      "|   7|  8|\"<p> so i'm tryin...|\n",
      "|   8|  9|\"<p> 1) i created...|\n",
      "|   9| 10|\"<p> does anyone ...|\n",
      "|  10| 11|\"<p> i'm using xa...|\n",
      "|  11| 12|<p> in sql server...|\n",
      "|  12| 13|\"<p> when i use p...|\n",
      "|  13| 14|\"<p> i need to sc...|\n",
      "|  14| 15|\"<p> i have added...|\n",
      "|  15| 16|<p> i have a mong...|\n",
      "|  16| 17|\"<p> okay so i'm ...|\n",
      "|  17| 18|\"<p> i'm fetching...|\n",
      "|  18| 19|\"<p> when i'm pla...|\n",
      "+----+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| _c0|               words|         rawFeatures|            features|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|null|[<p>, i, am, tryi...|(4096,[5,11,31,32...|(4096,[5,11,31,32...|\n",
      "|   0|[<p>, i, understa...|(4096,[3,25,45,12...|(4096,[3,25,45,12...|\n",
      "|   1|[\"<p>, i, am, mak...|(4096,[8,50,170,1...|(4096,[8,50,170,1...|\n",
      "|   2|[<p>, i, am, tryi...|(4096,[133,244,26...|(4096,[133,244,26...|\n",
      "|   3|[\"<p>, when, i, a...|(4096,[16,35,45,5...|(4096,[16,35,45,5...|\n",
      "|   4|[<p>, i, am, tryi...|(4096,[35,159,244...|(4096,[35,159,244...|\n",
      "|   5|[<p>, wanted, to,...|(4096,[103,134,15...|(4096,[103,134,15...|\n",
      "|   6|[\"<p>, i'm, wonde...|(4096,[8,56,77,11...|(4096,[8,56,77,11...|\n",
      "|   7|[\"<p>, so, i'm, t...|(4096,[3,7,8,16,4...|(4096,[3,7,8,16,4...|\n",
      "|   8|[\"<p>, 1), i, cre...|(4096,[18,45,96,1...|(4096,[18,45,96,1...|\n",
      "|   9|[\"<p>, does, anyo...|(4096,[54,90,244,...|(4096,[54,90,244,...|\n",
      "|  10|[\"<p>, i'm, using...|(4096,[7,46,56,90...|(4096,[7,46,56,90...|\n",
      "|  11|[<p>, in, sql, se...|(4096,[8,109,179,...|(4096,[8,109,179,...|\n",
      "|  12|[\"<p>, when, i, u...|(4096,[5,8,14,28,...|(4096,[5,8,14,28,...|\n",
      "|  13|[\"<p>, i, need, t...|(4096,[8,45,59,76...|(4096,[8,45,59,76...|\n",
      "|  14|[\"<p>, i, have, a...|(4096,[7,244,389,...|(4096,[7,244,389,...|\n",
      "|  15|[<p>, i, have, a,...|(4096,[8,84,131,2...|(4096,[8,84,131,2...|\n",
      "|  16|[\"<p>, okay, so, ...|(4096,[4,8,22,45,...|(4096,[4,8,22,45,...|\n",
      "|  17|[\"<p>, i'm, fetch...|(4096,[8,77,86,15...|(4096,[8,77,86,15...|\n",
      "|  18|[\"<p>, when, i'm,...|(4096,[45,79,154,...|(4096,[45,79,154,...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# if __name__ == \"__main__\":\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"TfIdf Example\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sentenceData = spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    ".load(\"pw_csmall.csv\")\n",
    "\n",
    "sentenceData.show()\n",
    "\n",
    "# sentenceData = spark.createDataFrame([\n",
    "#     (0.0, \"Welcome to TutorialKart.\"),\n",
    "#     (0.0, \"Learn Spark at TutorialKart.\"),\n",
    "#     (1.0, \"Spark Mllib has TF-IDF.\")\n",
    "# ], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"_c2\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**12)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "idfModel.save('temp/idf-model')\n",
    "\n",
    "rescaledData.select(\"_c0\", \"words\", \"rawFeatures\",\"features\").show()\n",
    " \n",
    "# spark.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.__version__\n",
    "# import pyspark\n",
    "# pyspark.__version__\n",
    "# SparkSession.version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \n"
     ]
    }
   ],
   "source": [
    "sample = rescaledData.take(1)[0]['_c2']\n",
    "print(sample)\n",
    "# # print(type(sample[0]['rawFeatures']))\n",
    "# print(sample.dot(sample))\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"_c2\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.19199301929721657, 0.18590004487687317, 0.18255778146344773, 0.1746916387774224]\n"
     ]
    }
   ],
   "source": [
    "# similarities = rescaledData.select(\"features\").rdd.map(lambda v: v)#/(v[0].norm(2)*candidate[0].norm(2)))\n",
    "# s = similarities.collect()\n",
    "def cos(a, b):\n",
    "    print(a[0].norm(2))\n",
    "    return a[0].dot(b)/(a[0].norm(2)*b.norm(2))\n",
    "# def cos(a,b):\n",
    "#     print(a[0].dot(b))\n",
    "#     return a\n",
    "\n",
    "sim = rescaledData.select(\"features\").rdd.map(lambda x: cos(x, sample)).sortBy(lambda x: -x).take(5)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel.write().overwrite().save('idf/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.write.parquet(\"test2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file and throw it in\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, IDFModel\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "    \n",
    "text = \"<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \"\n",
    "# if __name__ == \"__main__\":\n",
    "# spark = SparkSession\\\n",
    "#     .builder\\\n",
    "#     .appName(\"TfIdf Example\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sentenceData = spark.createDataFrame([(0.0, text),],['label','sentence'])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**12)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idfPath = 'idf/'\n",
    "\n",
    "modelPath = \"temp/idf-model\"\n",
    "# model.save(modelPath)\n",
    "loadedModel = IDFModel.load(modelPath)\n",
    "sample = loadedModel.transform(featurizedData)\n",
    "\n",
    "data = spark.read.format(\"parquet\").load(\"test2.parquet\")\n",
    "# loadedModel.transform(df).head().idf == model.transform(df).head().idf\n",
    "\n",
    "# loadedIdf = IDF.load(idfPath)\n",
    "# output = idfModel.transform(featurizedData)\n",
    "# output.show()\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# rescaledData.select(\"_c0\", \"words\", \"rawFeatures\",\"features\").show()\n",
    "\n",
    "#     (0.0, \"Welcome to TutorialKart.\"),\n",
    "#     (0.0, \"Learn Spark at TutorialKart.\"),\n",
    "#     (1.0, \"Spark Mllib has TF-IDF.\")\n",
    "# ], [\"label\", \"sentence\"])\n",
    "\n",
    "# sentenceData = spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# .load(\"pw_csmall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|<p> i am trying t...|[<p>, i, am, tryi...|(4096,[5,11,31,32...|(4096,[5,11,31,32...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "(4096,[5,11,31,32,57,101,159,179,189,191,192,240,244,293,365,382,392,403,404,406,500,566,630,641,658,672,695,721,829,835,877,933,950,987,991,1004,1025,1075,1089,1126,1186,1241,1265,1267,1269,1309,1310,1329,1347,1372,1377,1411,1414,1432,1438,1447,1455,1531,1565,1575,1591,1682,1737,1775,1804,1834,1877,1902,1919,2024,2071,2072,2081,2088,2130,2135,2151,2189,2213,2227,2248,2275,2312,2380,2391,2423,2439,2485,2488,2489,2492,2520,2526,2561,2562,2575,2618,2623,2647,2651,2666,2702,2711,2750,2760,2775,2776,2789,2791,2818,2832,2833,2855,2899,3030,3041,3051,3058,3111,3157,3172,3209,3270,3282,3296,3305,3331,3371,3377,3392,3420,3458,3465,3530,3556,3601,3619,3620,3673,3678,3709,3716,3742,3766,3772,3826,3874,3886,3888,3906,3937,3959,4017,4044],[4.42484663185681,4.42484663185681,3.172083663361442,4.2017030805426,4.511858008846439,2.8667020138102597,1.7723555036951266,2.997730276216664,3.4440173788450834,1.8038078077442294,4.344803924183274,6.341888208144207,0.6508214830391391,4.076539937588594,2.147579346847054,4.019381523748645,1.5437772666229586,3.6908774567766094,1.063314506587086,14.327788227574619,3.5085558999826545,3.2988353690005856,3.383392757028649,3.0596056799347515,3.4757660771596637,2.9024200964123392,4.076539937588594,4.607168188650764,2.1561630905384455,2.335042303141427,4.344803924183274,4.137164559405029,4.511858008846439,3.1030907918744903,1.4804076526903693,0.9831844282559518,2.4671020251544937,7.155097542939212,3.8187108282864943,6.417205971537436,1.0631891695024356,1.7392692866066584,3.6816981588491573,4.48459898556984,0.9956757717712901,10.23598762483351,4.830311739964974,3.4757660771596637,2.652513945726222,2.4789364828014966,3.6516567436233283,1.750697982430281,6.441747655061747,2.7988794174714986,1.023191411548257,4.951614716751893,4.2017030805426,3.6139164156404813,1.6455707217993054,2.2276220545205905,2.549927356951121,4.137164559405029,7.5485181314313206,7.901117394056473,4.344803924183274,3.0179329835341835,1.641895122581482,4.137164559405029,4.830311739964974,1.1508515077675294,3.304515819234057,3.4757660771596637,4.511858008846439,2.4438451629902262,2.594742330573881,1.037693265338048,2.1912544103497154,4.019381523748645,18.43666476525375,3.6516567436233283,4.2017030805426,10.33205213653525,23.79188581487022,18.428672754603056,3.5085558999826545,4.270695952029551,29.527019654212875,8.274329118810059,2.8667020138102597,4.2017030805426,1.722367475804055,2.335042303141427,6.8264914403566594,2.6756467770475507,4.607168188650764,3.542457451658336,4.019381523748645,3.9653143024783697,1.548461115935385,0.18812590152912093,4.270695952029551,3.6139164156404813,3.7316994512968646,4.830311739964974,12.585413557634354,4.607168188650764,3.9653143024783697,3.0460513421398776,2.7666185552532774,4.607168188650764,3.0385522707369192,0.7861947242444419,4.607168188650764,3.9653143024783697,3.2721671219184243,3.7316994512968646,37.217320327315136,10.532348650515019,7.2278328312809625,22.984188876437507,0.9643326730382349,6.080854077175318,2.7666185552532774,1.3375992494670454,4.019381523748645,1.9550161883935986,6.2126680278911905,3.125563647726549,4.076539937588594,1.9735481954367646,11.69808434886493,3.8356244994703337,4.019381523748645,3.3262343431887,2.6902455764687034,0.25560076146159144,1.9049737812086138,8.84969326371362,9.376690943179646,3.2461916355151637,10.627372354975007,4.830311739964974,1.3567936967231926,4.511858008846439,3.125563647726549,2.0975689262723924,1.251238241148016,4.137164559405029,5.698620542196782,4.344803924183274,0.9077345139716589,1.3529252199452724,4.2017030805426,2.5854200762127455])\n"
     ]
    }
   ],
   "source": [
    "sample.show()\n",
    "sample2 = sample.take(1)[0]['features']\n",
    "print(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.19199301929721657, 0.18590004487687317, 0.18255778146344773, 0.1746916387774224]\n"
     ]
    }
   ],
   "source": [
    "def cos(a, b):\n",
    "    print(a[0].norm(2))\n",
    "    return a[0].dot(b)/(a[0].norm(2)*b.norm(2))\n",
    "\n",
    "sim = data.select(\"features\").rdd.map(lambda x: cos(x, sample2)).sortBy(lambda x: -x).take(5)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-55e9e0f59665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# df2 = df.withColumn('strFeatures', col('features').cast(\"string\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# df2.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# df.write.option(\"delimiter\", \"\\t\").csv('test.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'txt'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparse_format_udf = udf(lambda x: ','.join([str(elem) for elem in x], StringType()))\n",
    "\n",
    "def array_to_string(my_list):\n",
    "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
    "\n",
    "array_to_string_udf = udf(array_to_string, StringType())\n",
    "\n",
    "dense_format_udf = udf(lambda x: str(x), StringType())\n",
    "\n",
    "\n",
    "\n",
    "# df.show()\n",
    "# df = rescaledData.withColumn('features', dense_format_udf(col('features').cast(\"string\")))\n",
    "# df2 = df.withColumn('strFeatures', col('features').cast(\"string\"))\n",
    "# df2.show()\n",
    "# df.write.option(\"delimiter\",'\\t').csv('test.txt')\n",
    "# df.write.option(\"delimiter\", \"\\t\").csv('test.csv')\n",
    "\n",
    "\n",
    "# df = rescaledData.withColumn('features2', sparse_format_udf(col('features')))\n",
    "# df.show()\n",
    "# df2 = df.withColumn('features', lit(\"features\").cast(\"string\"))\n",
    "# df = df.withColumn('features',array_to_string_udf(d[\"features\"]))\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df2.write.option(\"delimiter\", \"\\t\").csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 0 ns, total: 3.14 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%time d = spark.read.parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Learn Spark at Tu...|[learn, spark, at...|(20,[4,5,15,16],[...|(20,[4,5,15,16],[...|\n",
      "|  0.0|Welcome to Tutori...|[welcome, to, tut...|(20,[4,8,9],[1.0,...|(20,[4,8,9],[0.28...|\n",
      "|  1.0|Spark Mllib has T...|[spark, mllib, ha...|(20,[0,1,5,14],[1...|(20,[0,1,5,14],[0...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF, IDFModel\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "df = spark.createDataFrame([(DenseVector([1.0, 2.0]),),\n",
    "     (DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [\"tf\"])\n",
    "idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n",
    "model = idf.fit(df)\n",
    "model.idf\n",
    "# DenseVector([0.0, 0.0])\n",
    "model.transform(df).head().idf\n",
    "# DenseVector([0.0, 0.0])\n",
    "idf.setParams(outputCol=\"freqs\").fit(df).transform(df).collect()[1].freqs\n",
    "# DenseVector([0.0, 0.0])\n",
    "params = {idf.minDocFreq: 1, idf.outputCol: \"vector\"}\n",
    "idf.fit(df, params).transform(df).head().vector\n",
    "# DenseVector([0.2877, 0.0])\n",
    "# temp_path = '/tp'\n",
    "idfPath = \"tp/idf\"\n",
    "idf.save(idfPath)\n",
    "loadedIdf = IDF.load(idfPath)\n",
    "loadedIdf.getMinDocFreq() == idf.getMinDocFreq()\n",
    "\n",
    "modelPath = \"tp/idf-model\"\n",
    "model.save(modelPath)\n",
    "loadedModel = IDFModel.load(modelPath)\n",
    "loadedModel.transform(df).head().idf == model.transform(df).head().idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF.save(hashingTFPath)\n",
    "# >>> loadedHashingTF = HashingTF.load(hashingTFPath)\n",
    "# >>> loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings list redis cassandar\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
