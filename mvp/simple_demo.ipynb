{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+------------------+------------+-------------+--------------------+--------------------+--------------+--------------------+--------------------+------------------------+-------------------+------------------+-------------+------------+-----+--------------------+----------+\n",
      "|_c0|      id|               title|                body|accepted_answer_id|answer_count|comment_count|community_owned_date|       creation_date|favorite_count|  last_activity_date|      last_edit_date|last_editor_display_name|last_editor_user_id|owner_display_name|owner_user_id|post_type_id|score|                tags|view_count|\n",
      "+---+--------+--------------------+--------------------+------------------+------------+-------------+--------------------+--------------------+--------------+--------------------+--------------------+------------------------+-------------------+------------------+-------------+------------+-----+--------------------+----------+\n",
      "|  0|23280293|Jasper report dat...|<p>I am trying to...|        23373319.0|           1|            2|                null|2014-04-24 21:33:...|          null|2014-05-01 13:37:...|2014-05-01 13:37:...|                    null|           321731.0|              null|    1214943.0|           1|    0|filter|jasper-rep...|       266|\n",
      "|  1| 5787776|A worthy develope...|<p>I understand p...|              null|           8|            9|                null|2011-04-26 08:28:...|          40.0|2013-03-19 01:57:...|2013-03-19 01:57:...|                    null|           772853.0|              null|      50841.0|           1|   95|paypal|payment-ga...|     10505|\n",
      "|  2|51899406|Why does adding a...|<p>I am making a ...|              null|           1|            0|                null|2018-08-17 16:17:...|          null|2018-08-17 17:08:...|2018-08-17 17:08:...|                    null|           241631.0|              null|    3662349.0|           1|    3|c++|c++11|templat...|        53|\n",
      "|  3|29219176|Java Math.pow(x,2...|<p>I am trying to...|        29232947.0|           1|            8|                null|2015-03-23 19:55:...|           2.0|2016-07-19 21:48:...|2016-07-19 21:48:...|                    null|          2753863.0|              null|    2739693.0|           1|    5|    java|performance|       427|\n",
      "|  4|45545053|rioBufferWrite fu...|<p>When I am read...|              null|           0|            6|                null|2017-08-07 10:46:...|          null|2017-08-07 10:57:...|2017-08-07 10:57:...|                    null|          8051589.0|              null|    7700616.0|           1|    1|             c|redis|        30|\n",
      "|  5|49452670|Why Does PHP say ...|<p>I am trying to...|              null|           0|            1|                null|2018-03-23 15:00:...|          null|2018-03-23 15:16:...|2018-03-23 15:16:...|                    null|           223863.0|              null|     223863.0|           1|    0|                 php|        53|\n",
      "|  6|30640113|Strange interacti...|<p>Wanted to get ...|              null|           0|            8|                null|2015-06-04 09:29:...|          null|2015-06-04 09:31:...|2015-06-04 09:31:...|                    null|          3001626.0|              null|    3571832.0|           1|    4|                   r|       225|\n",
      "|  7|42645929|Highcharts - Cust...|<p>I'm wondering ...|              null|           1|            5|                null|2017-03-07 10:34:...|          null|2017-03-08 15:12:...|                null|                    null|               null|              null|    5242744.0|           1|    0|          highcharts|       627|\n",
      "|  8|25021096|Android ListView ...|<p>So, I'm trying...|        25027323.0|           2|            3|                null|2014-07-29 17:08:...|          null|2014-08-02 15:17:...|                null|                    null|               null|              null|     381288.0|           1|    0|java|android|list...|       216|\n",
      "|  9| 7252753|NSTimer calls the...|<p>1) I created a...|         7252789.0|           1|            0|                null|2011-08-31 05:04:...|          null|2011-08-31 05:24:...|                null|                    null|               null|              null|     372446.0|           1|    1|         ios|nstimer|      1577|\n",
      "| 10|15182568|Using Movie Maker...|<p>Does anyone kn...|        15182664.0|           2|            0|                null|2013-03-03 04:53:...|          null|2016-03-30 16:20:...|                null|                    null|               null|              null|     625952.0|           1|    0|windows|command-l...|      2445|\n",
      "| 11|31758045|ajax post data un...|<p>I'm using xamp...|              null|           2|            3|                null|2015-08-01 03:55:...|          null|2018-11-21 05:30:...|2015-08-01 04:06:...|                    null|          5141010.0|              null|    5141010.0|           1|    0|javascript|jquery...|      5916|\n",
      "| 12|51764666|SQL Server 2018 M...|<p>In SQL Server ...|              null|           0|            4|                null|2018-08-09 10:30:...|          null|2018-08-09 11:58:...|2018-08-09 11:58:...|                    null|            13302.0|              null|    2216540.0|           1|    0|          sql-server|        43|\n",
      "| 13|45134867|CSS & HTML how to...|<p>When I use pin...|        45135241.0|           1|            6|                null|2017-07-17 01:34:...|           1.0|2017-07-17 03:05:...|2017-07-17 01:38:...|                    null|           877671.0|              null|    7759287.0|           1|    1|       html|css|css3|       339|\n",
      "| 14|25787394|Scrape content wi...|<p>I need to scra...|              null|           0|           12|                null|2014-09-11 12:24:...|           1.0|2014-09-17 21:01:...|2014-09-17 20:08:...|                    null|           472495.0|              null|     940491.0|           1|    0|php|ajax|html-par...|      1151|\n",
      "| 15|41955896|Dropins Drobox pl...|<p>I have added t...|        41965709.0|           1|            0|                null|2017-01-31 11:01:...|          null|2017-01-31 19:15:...|                null|                    null|               null|              null|     678833.0|           1|    0|javascript|dropbo...|        88|\n",
      "| 16|19559952|Export MongoDB da...|<p>I have a Mongo...|        19561401.0|           1|            1|                null|2013-10-24 07:45:...|          null|2013-10-24 08:59:...|                null|                    null|               null|              null|      57630.0|           1|    0|      mongodb|ubuntu|        91|\n",
      "| 17|30225817|How to store symm...|<p>Okay so I'm wo...|              null|           3|            4|                null|2015-05-13 21:53:...|          null|2015-05-14 21:45:...|2015-05-14 03:13:...|                    null|          1322972.0|              null|    4789656.0|           1|    1|c++|arrays|pointe...|       113|\n",
      "| 18|53535220|Python: KeyError ...|<p>I'm fetching i...|        53535558.0|           1|            4|                null|2018-11-29 09:03:...|          null|2018-11-29 09:22:...|2018-11-29 09:08:...|                    null|          5720196.0|              null|    3769964.0|           1|    0|python|json|pytho...|       106|\n",
      "| 19|22411997|Win8 GridView ite...|<p>When I'm placi...|        22455353.0|           2|            1|                null|2014-03-14 17:33:...|          null|2014-03-17 13:17:...|2014-03-15 17:07:...|                    null|           168868.0|              null|    1192466.0|           1|    1|   c#|wpf|winrt-xaml|        66|\n",
      "+---+--------+--------------------+--------------------+------------------+------------+-------------+--------------------+--------------------+--------------+--------------------+--------------------+------------------------+-------------------+------------------+-------------+------------+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|_c0|               words|         rawFeatures|            features|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|[<p>i, am, trying...|(65536,[672,721,2...|(65536,[672,721,2...|\n",
      "|  1|[<p>i, understand...|(65536,[2606,3727...|(65536,[2606,3727...|\n",
      "|  2|[<p>i, am, making...|(65536,[170,4797,...|(65536,[170,4797,...|\n",
      "|  3|[<p>i, am, trying...|(65536,[543,731,1...|(65536,[543,731,1...|\n",
      "|  4|[<p>when, i, am, ...|(65536,[672,2848,...|(65536,[672,2848,...|\n",
      "|  5|[<p>i, am, trying...|(65536,[2334,2565...|(65536,[2334,2565...|\n",
      "|  6|[<p>wanted, to, g...|(65536,[731,2497,...|(65536,[731,2497,...|\n",
      "|  7|[<p>i'm, wonderin...|(65536,[602,1760,...|(65536,[602,1760,...|\n",
      "|  8|[<p>so,, i'm, try...|(65536,[34,330,43...|(65536,[34,330,43...|\n",
      "|  9|[<p>1), i, create...|(65536,[1198,1536...|(65536,[1198,1536...|\n",
      "| 10|[<p>does, anyone,...|(65536,[54,2848,4...|(65536,[54,2848,4...|\n",
      "| 11|[<p>i'm, using, x...|(65536,[1538,2018...|(65536,[1538,2018...|\n",
      "| 12|[<p>in, sql, serv...|(65536,[966,1020,...|(65536,[966,1020,...|\n",
      "| 13|[<p>when, i, use,...|(65536,[805,828,1...|(65536,[805,828,1...|\n",
      "| 14|[<p>i, need, to, ...|(65536,[395,1536,...|(65536,[395,1536,...|\n",
      "| 15|[<p>i, have, adde...|(65536,[546,5624,...|(65536,[546,5624,...|\n",
      "| 16|[<p>i, have, a, m...|(65536,[353,637,3...|(65536,[353,637,3...|\n",
      "| 17|[<p>okay, so, i'm...|(65536,[170,418,7...|(65536,[170,418,7...|\n",
      "| 18|[<p>i'm, fetching...|(65536,[166,284,2...|(65536,[166,284,2...|\n",
      "| 19|[<p>when, i'm, pl...|(65536,[1301,1669...|(65536,[1301,1669...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# if __name__ == \"__main__\":\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"TfIdf Example\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sentenceData = spark.read.csv(\"pq_dirty.csv\", header=True, multiLine=True, escape='\"')\n",
    "\n",
    "sentenceData.show()\n",
    "\n",
    "# sentenceData = spark.createDataFrame([\n",
    "#     (0.0, \"Welcome to TutorialKart.\"),\n",
    "#     (0.0, \"Learn Spark at TutorialKart.\"),\n",
    "#     (1.0, \"Spark Mllib has TF-IDF.\")\n",
    "# ], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**16)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "idfModel.save('temp/idf-model')\n",
    "\n",
    "rescaledData.select(\"_c0\", \"words\", \"rawFeatures\",\"features\").show()\n",
    " \n",
    "# spark.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.__version__\n",
    "# import pyspark\n",
    "# pyspark.__version__\n",
    "# SparkSession.version\n",
    "# spark.version\n",
    "# get at the sparse vectors\n",
    "a = rescaledData.select('features').take(10)\n",
    "b = a[0][0]\n",
    "dir(b)\n",
    "s, v, i = b.size, b.values, b.indices\n",
    "# print(type(b.values))\n",
    "# c = b.asDict()\n",
    "# print(c)\n",
    "import numpy as np\n",
    "# aa = np.random.rand(50,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# ee = csr_matrix((b.values, (b.indices,1)), shape=(b.size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "# %timeit dd = SparseVector(s, i, v)\n",
    "# print(i) o shiiiiit it can straight up read in bytes\n",
    "test = \"|\"\n",
    "test = test.join([str(s),str(v),str(i)])\n",
    "# print(test)\n",
    "# print(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# bb = np.fromstring(tl[1])\n",
    "ss = '[1,2,3,4,5]'\n",
    "aa = np.fromstring(ss, sep=',')\n",
    "print(aa)\n",
    "# print(tl[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "string size must be a multiple of element size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-7ada4942bc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(tl)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a = np.frombuffer(bytes(tl[1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: string size must be a multiple of element size"
     ]
    }
   ],
   "source": [
    "tl = test.split('|')\n",
    "# print(list(tl[1]))\n",
    "# print(tl)\n",
    "# a = np.frombuffer(bytes(tl[1]))\n",
    "dd = SparseVector(int(tl[0]), np.fromstring(tl[1]), np.fromstring(tl[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 5000 into shape (50,50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-dfb5b7cf0310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdiy_dumped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiy_dumped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-dfb5b7cf0310>\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>II'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 5000 into shape (50,50)"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "def serialize(mat):\n",
    "    n_rows = len(mat)\n",
    "    n_columns = len(mat[0])\n",
    "#     shape = struct.pack('>II', n_rows, n_columns)\n",
    "    return mat.tobytes()\n",
    "\n",
    "def deserialize(data):\n",
    "#     n_rows, n_columns = struct.unpack('>II', data[:8])\n",
    "    mat = np.frombuffer(data, dtype=np.float32, offset=8)\n",
    "    mat.shape = (n_rows, n_columns)\n",
    "    return mat\n",
    "\n",
    "diy_dumped = serialize(aa)\n",
    "deserialize(diy_dumped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \n"
     ]
    }
   ],
   "source": [
    "sample = rescaledData.take(1)[0]['_c2']\n",
    "print(sample)\n",
    "# # print(type(sample[0]['rawFeatures']))\n",
    "# print(sample.dot(sample))\n",
    "\n",
    "# tokenizer = Tokenizer(inputCol=\"_c2\", outputCol=\"words\")\n",
    "# wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.19199301929721657, 0.18590004487687317, 0.18255778146344773, 0.1746916387774224]\n"
     ]
    }
   ],
   "source": [
    "# similarities = rescaledData.select(\"features\").rdd.map(lambda v: v)#/(v[0].norm(2)*candidate[0].norm(2)))\n",
    "# s = similarities.collect()\n",
    "def cos(a, b):\n",
    "    print(a[0].norm(2))\n",
    "    return a[0].dot(b)/(a[0].norm(2)*b.norm(2))\n",
    "# def cos(a,b):\n",
    "#     print(a[0].dot(b))\n",
    "#     return a\n",
    "\n",
    "sim = rescaledData.select(\"features\").rdd.map(lambda x: cos(x, sample)).sortBy(lambda x: -x).take(5)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel.write().overwrite().save('idf/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.write.parquet(\"test2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file and throw it in\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, IDFModel\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "    \n",
    "text = \"<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \"\n",
    "# if __name__ == \"__main__\":\n",
    "# spark = SparkSession\\\n",
    "#     .builder\\\n",
    "#     .appName(\"TfIdf Example\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "sentenceData = spark.createDataFrame([(0.0, text),],['label','sentence'])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2**12)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idfPath = 'idf/'\n",
    "\n",
    "modelPath = \"temp/idf-model\"\n",
    "# model.save(modelPath)\n",
    "loadedModel = IDFModel.load(modelPath)\n",
    "sample = loadedModel.transform(featurizedData)\n",
    "\n",
    "data = spark.read.format(\"parquet\").load(\"test2.parquet\")\n",
    "# loadedModel.transform(df).head().idf == model.transform(df).head().idf\n",
    "\n",
    "# loadedIdf = IDF.load(idfPath)\n",
    "# output = idfModel.transform(featurizedData)\n",
    "# output.show()\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# rescaledData.select(\"_c0\", \"words\", \"rawFeatures\",\"features\").show()\n",
    "\n",
    "#     (0.0, \"Welcome to TutorialKart.\"),\n",
    "#     (0.0, \"Learn Spark at TutorialKart.\"),\n",
    "#     (1.0, \"Spark Mllib has TF-IDF.\")\n",
    "# ], [\"label\", \"sentence\"])\n",
    "\n",
    "# sentenceData = spark.read.format(\"csv\").option(\"header\",\"false\")\\\n",
    "# .load(\"pw_csmall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|<p> i am trying t...|[<p>, i, am, tryi...|(4096,[5,11,31,32...|(4096,[5,11,31,32...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "(4096,[5,11,31,32,57,101,159,179,189,191,192,240,244,293,365,382,392,403,404,406,500,566,630,641,658,672,695,721,829,835,877,933,950,987,991,1004,1025,1075,1089,1126,1186,1241,1265,1267,1269,1309,1310,1329,1347,1372,1377,1411,1414,1432,1438,1447,1455,1531,1565,1575,1591,1682,1737,1775,1804,1834,1877,1902,1919,2024,2071,2072,2081,2088,2130,2135,2151,2189,2213,2227,2248,2275,2312,2380,2391,2423,2439,2485,2488,2489,2492,2520,2526,2561,2562,2575,2618,2623,2647,2651,2666,2702,2711,2750,2760,2775,2776,2789,2791,2818,2832,2833,2855,2899,3030,3041,3051,3058,3111,3157,3172,3209,3270,3282,3296,3305,3331,3371,3377,3392,3420,3458,3465,3530,3556,3601,3619,3620,3673,3678,3709,3716,3742,3766,3772,3826,3874,3886,3888,3906,3937,3959,4017,4044],[4.42484663185681,4.42484663185681,3.172083663361442,4.2017030805426,4.511858008846439,2.8667020138102597,1.7723555036951266,2.997730276216664,3.4440173788450834,1.8038078077442294,4.344803924183274,6.341888208144207,0.6508214830391391,4.076539937588594,2.147579346847054,4.019381523748645,1.5437772666229586,3.6908774567766094,1.063314506587086,14.327788227574619,3.5085558999826545,3.2988353690005856,3.383392757028649,3.0596056799347515,3.4757660771596637,2.9024200964123392,4.076539937588594,4.607168188650764,2.1561630905384455,2.335042303141427,4.344803924183274,4.137164559405029,4.511858008846439,3.1030907918744903,1.4804076526903693,0.9831844282559518,2.4671020251544937,7.155097542939212,3.8187108282864943,6.417205971537436,1.0631891695024356,1.7392692866066584,3.6816981588491573,4.48459898556984,0.9956757717712901,10.23598762483351,4.830311739964974,3.4757660771596637,2.652513945726222,2.4789364828014966,3.6516567436233283,1.750697982430281,6.441747655061747,2.7988794174714986,1.023191411548257,4.951614716751893,4.2017030805426,3.6139164156404813,1.6455707217993054,2.2276220545205905,2.549927356951121,4.137164559405029,7.5485181314313206,7.901117394056473,4.344803924183274,3.0179329835341835,1.641895122581482,4.137164559405029,4.830311739964974,1.1508515077675294,3.304515819234057,3.4757660771596637,4.511858008846439,2.4438451629902262,2.594742330573881,1.037693265338048,2.1912544103497154,4.019381523748645,18.43666476525375,3.6516567436233283,4.2017030805426,10.33205213653525,23.79188581487022,18.428672754603056,3.5085558999826545,4.270695952029551,29.527019654212875,8.274329118810059,2.8667020138102597,4.2017030805426,1.722367475804055,2.335042303141427,6.8264914403566594,2.6756467770475507,4.607168188650764,3.542457451658336,4.019381523748645,3.9653143024783697,1.548461115935385,0.18812590152912093,4.270695952029551,3.6139164156404813,3.7316994512968646,4.830311739964974,12.585413557634354,4.607168188650764,3.9653143024783697,3.0460513421398776,2.7666185552532774,4.607168188650764,3.0385522707369192,0.7861947242444419,4.607168188650764,3.9653143024783697,3.2721671219184243,3.7316994512968646,37.217320327315136,10.532348650515019,7.2278328312809625,22.984188876437507,0.9643326730382349,6.080854077175318,2.7666185552532774,1.3375992494670454,4.019381523748645,1.9550161883935986,6.2126680278911905,3.125563647726549,4.076539937588594,1.9735481954367646,11.69808434886493,3.8356244994703337,4.019381523748645,3.3262343431887,2.6902455764687034,0.25560076146159144,1.9049737812086138,8.84969326371362,9.376690943179646,3.2461916355151637,10.627372354975007,4.830311739964974,1.3567936967231926,4.511858008846439,3.125563647726549,2.0975689262723924,1.251238241148016,4.137164559405029,5.698620542196782,4.344803924183274,0.9077345139716589,1.3529252199452724,4.2017030805426,2.5854200762127455])\n"
     ]
    }
   ],
   "source": [
    "sample.show()\n",
    "sample2 = sample.take(1)[0]['features']\n",
    "print(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.19199301929721657, 0.18590004487687317, 0.18255778146344773, 0.1746916387774224]\n"
     ]
    }
   ],
   "source": [
    "def cos(a, b):\n",
    "    print(a[0].norm(2))\n",
    "    return a[0].dot(b)/(a[0].norm(2)*b.norm(2))\n",
    "\n",
    "sim = data.select(\"features\").rdd.map(lambda x: cos(x, sample2)).sortBy(lambda x: -x).take(5)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-55e9e0f59665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# df2 = df.withColumn('strFeatures', col('features').cast(\"string\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# df2.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# df.write.option(\"delimiter\", \"\\t\").csv('test.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'txt'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sparse_format_udf = udf(lambda x: ','.join([str(elem) for elem in x], StringType()))\n",
    "\n",
    "def array_to_string(my_list):\n",
    "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
    "\n",
    "array_to_string_udf = udf(array_to_string, StringType())\n",
    "\n",
    "dense_format_udf = udf(lambda x: str(x), StringType())\n",
    "\n",
    "\n",
    "\n",
    "# df.show()\n",
    "# df = rescaledData.withColumn('features', dense_format_udf(col('features').cast(\"string\")))\n",
    "# df2 = df.withColumn('strFeatures', col('features').cast(\"string\"))\n",
    "# df2.show()\n",
    "# df.write.option(\"delimiter\",'\\t').csv('test.txt')\n",
    "# df.write.option(\"delimiter\", \"\\t\").csv('test.csv')\n",
    "\n",
    "\n",
    "# df = rescaledData.withColumn('features2', sparse_format_udf(col('features')))\n",
    "# df.show()\n",
    "# df2 = df.withColumn('features', lit(\"features\").cast(\"string\"))\n",
    "# df = df.withColumn('features',array_to_string_udf(d[\"features\"]))\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df2.write.option(\"delimiter\", \"\\t\").csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 0 ns, total: 3.14 ms\n",
      "Wall time: 153 ms\n"
     ]
    }
   ],
   "source": [
    "%time d = spark.read.parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Learn Spark at Tu...|[learn, spark, at...|(20,[4,5,15,16],[...|(20,[4,5,15,16],[...|\n",
      "|  0.0|Welcome to Tutori...|[welcome, to, tut...|(20,[4,8,9],[1.0,...|(20,[4,8,9],[0.28...|\n",
      "|  1.0|Spark Mllib has T...|[spark, mllib, ha...|(20,[0,1,5,14],[1...|(20,[0,1,5,14],[0...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF, IDFModel, Tokenizer\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "df = spark.createDataFrame([(DenseVector([1.0, 2.0]),),\n",
    "     (DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [\"tf\"])\n",
    "idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n",
    "model = idf.fit(df)\n",
    "model.idf\n",
    "# DenseVector([0.0, 0.0])\n",
    "model.transform(df).head().idf\n",
    "# DenseVector([0.0, 0.0])\n",
    "idf.setParams(outputCol=\"freqs\").fit(df).transform(df).collect()[1].freqs\n",
    "# DenseVector([0.0, 0.0])\n",
    "params = {idf.minDocFreq: 1, idf.outputCol: \"vector\"}\n",
    "idf.fit(df, params).transform(df).head().vector\n",
    "# DenseVector([0.2877, 0.0])\n",
    "# temp_path = '/tp'\n",
    "idfPath = \"tp/idf\"\n",
    "idf.save(idfPath)\n",
    "loadedIdf = IDF.load(idfPath)\n",
    "loadedIdf.getMinDocFreq() == idf.getMinDocFreq()\n",
    "\n",
    "modelPath = \"tp/idf-model\"\n",
    "model.save(modelPath)\n",
    "loadedModel = IDFModel.load(modelPath)\n",
    "loadedModel.transform(df).head().idf == model.transform(df).head().idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF.save(hashingTFPath)\n",
    "# >>> loadedHashingTF = HashingTF.load(hashingTFPath)\n",
    "# >>> loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings list redis cassandar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notFollowingList=List(9.8,7,6,3,1)\n",
    "df.filter(col(\"uid\").isin(notFollowingList:_*))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
