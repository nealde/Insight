{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv('ph.csv', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_history_type_id</th>\n",
       "      <th>revision_guid</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28369829</td>\n",
       "      <td>2012-08-24 08:18:33.29 UTC</td>\n",
       "      <td>12105586</td>\n",
       "      <td>1</td>\n",
       "      <td>95c45c9a-aff1-4b7c-adc2-76c727c6a930</td>\n",
       "      <td>1165423.0</td>\n",
       "      <td>Loading String Resources from A Different Path</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95514764</td>\n",
       "      <td>2015-07-20 18:28:11.287 UTC</td>\n",
       "      <td>31523754</td>\n",
       "      <td>1</td>\n",
       "      <td>c8ef2715-59bb-429b-ad39-25086ec4e123</td>\n",
       "      <td>5136201.0</td>\n",
       "      <td>System.Data.SqlClient.SqlException: Incorrect ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187966461</td>\n",
       "      <td>2018-12-17 09:16:42.123 UTC</td>\n",
       "      <td>53812067</td>\n",
       "      <td>1</td>\n",
       "      <td>72e5ae09-4edc-44f2-ae53-8eacda39a9f4</td>\n",
       "      <td>2105218.0</td>\n",
       "      <td>Do multiple jdbc input run simultaneously or s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8155373</td>\n",
       "      <td>2010-10-07 21:33:26.907 UTC</td>\n",
       "      <td>3886039</td>\n",
       "      <td>1</td>\n",
       "      <td>f419429b-9756-43ff-baf4-5927f589c73c</td>\n",
       "      <td>56621.0</td>\n",
       "      <td>\"Test with debugger\" in ReSharper?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40793970</td>\n",
       "      <td>2013-05-17 16:18:08.577 UTC</td>\n",
       "      <td>16613724</td>\n",
       "      <td>1</td>\n",
       "      <td>ef698524-da6f-4a1f-b0ba-310b38e730d7</td>\n",
       "      <td>2394636.0</td>\n",
       "      <td>jQuerymobile: Close a popup when opening a panel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                creation_date   post_id  post_history_type_id  \\\n",
       "0   28369829   2012-08-24 08:18:33.29 UTC  12105586                     1   \n",
       "1   95514764  2015-07-20 18:28:11.287 UTC  31523754                     1   \n",
       "2  187966461  2018-12-17 09:16:42.123 UTC  53812067                     1   \n",
       "3    8155373  2010-10-07 21:33:26.907 UTC   3886039                     1   \n",
       "4   40793970  2013-05-17 16:18:08.577 UTC  16613724                     1   \n",
       "\n",
       "                          revision_guid    user_id  \\\n",
       "0  95c45c9a-aff1-4b7c-adc2-76c727c6a930  1165423.0   \n",
       "1  c8ef2715-59bb-429b-ad39-25086ec4e123  5136201.0   \n",
       "2  72e5ae09-4edc-44f2-ae53-8eacda39a9f4  2105218.0   \n",
       "3  f419429b-9756-43ff-baf4-5927f589c73c    56621.0   \n",
       "4  ef698524-da6f-4a1f-b0ba-310b38e730d7  2394636.0   \n",
       "\n",
       "                                                text comment  \n",
       "0     Loading String Resources from A Different Path     NaN  \n",
       "1  System.Data.SqlClient.SqlException: Incorrect ...     NaN  \n",
       "2  Do multiple jdbc input run simultaneously or s...     NaN  \n",
       "3                 \"Test with debugger\" in ReSharper?     NaN  \n",
       "4   jQuerymobile: Close a popup when opening a panel     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['china', 'has', 'a', 'strong', 'economy', 'that', 'is', 'growing', 'at', 'a', 'rapid', 'pace.', 'however', 'politically', 'it', 'differs', 'greatly', 'from', 'the', 'us', 'economy.'], ['at', 'last,', 'china', 'seems', 'serious', 'about', 'confronting', 'an', 'endemic', 'problem:', 'domestic', 'violence', 'and', 'corruption.'], [\"japan's\", 'prime', 'minister,', 'shinzo', 'abe,', 'is', 'working', 'towards', 'healing', 'the', 'economic', 'turmoil', 'in', 'his', 'own', 'country', 'for', 'his', 'view', 'on', 'the', 'future', 'of', 'his', 'people.'], ['vladimir', 'putin', 'is', 'working', 'hard', 'to', 'fix', 'the', 'economy', 'in', 'russia', 'as', 'the', 'ruble', 'has', 'tumbled.'], [\"what's\", 'the', 'future', 'of', 'abenomics?', 'we', 'asked', 'shinzo', 'abe', 'for', 'his', 'views'], ['obama', 'has', 'eased', 'sanctions', 'on', 'cuba', 'while', 'accelerating', 'those', 'against', 'the', 'russian', 'economy,', 'even', 'as', 'the', \"ruble's\", 'value', 'falls', 'almost', 'daily.'], ['vladimir', 'putin', 'was', 'found', 'to', 'be', 'riding', 'a', 'horse,', 'again,', 'without', 'a', 'shirt', 'on', 'while', 'hunting', 'deer.', 'vladimir', 'putin', 'always', 'seems', 'so', 'serious', 'about', 'things', '-', 'even', 'riding', 'horses.']]\n"
     ]
    }
   ],
   "source": [
    "# demo from online:\n",
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "print(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.22 µs ± 89.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a = jaccard_similarity(tokenized_documents[2],tokenized_documents[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term-frequency inverse document frequency\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "#     print(tokenized_document.count(term))\n",
    "#     print(tokenized_document, term)\n",
    "    try:\n",
    "        return 1 + math.log(tokenized_document.count(term))\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9459101490553135\n",
      "1.336472236621213\n"
     ]
    }
   ],
   "source": [
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['abenomics?'])\n",
    "# 2.94591014906\n",
    "print(idf_values['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.55 ms, sys: 0 ns, total: 1.55 ms\n",
      "Wall time: 1.56 ms\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_representation = tfidf(all_documents)\n",
    "# print(tfidf_representation[0], document_0)\n",
    "# doc vector and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 1.336472236621213, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 3.8142592685777856, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8472978603872037, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 1.8472978603872037, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.31019096605521496, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18320378146489946, 0.0, 0.18320378146489946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.18320378146489946, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.23957330918096045, 0.23957330918096045, 0.0, 0.15022972156764192, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.15022972156764192, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.23957330918096045, 0.10868731908150663, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23957330918096045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(all_documents)\n",
    "print(tfidf_representation[0])\n",
    "print(sklearn_representation.toarray()[0].tolist())\n",
    "print(document_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System.Data.SqlClient.SqlException: Incorrect syntax near the keyword 'FROM'\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neal/miniconda3/envs/keras/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d2 = pd.read_csv('posts_questions.csv', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "#     print(type(line))\n",
    "    line = line.lower().replace(\"\\n\",\" \").replace(\"\\r\",\"\").replace(',',\"\").replace(\">\",\"> \")\n",
    "    return line\n",
    "# clean(d2['body'][0])\n",
    "# d2.to_csv('pq2.csv', sep='~')\n",
    "# col = []\n",
    "# for text in d2['body']:\n",
    "#     col.append(clean(text))\n",
    "# d2['cleaned'] = col\n",
    "# print(d2['cleaned'][0])\n",
    "# d2['cleaned'].to_csv('pq_clean.csv')\n",
    "import pandas as pd\n",
    "d2 = pd.read_csv(\"pq_clean.csv\", names=['index','body'])\n",
    "# print(d2['body'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.head()\n",
    "# d = 5\n",
    "def word_count(row):\n",
    "    return len(row.split(\" \"))\n",
    "#     print(row)\n",
    "#     l = row.split(',')[2]\n",
    "#     print(l)\n",
    "#     return len(l.split(\" \"))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "def process(filename):\n",
    "    sc = SparkContext(appName=\"Text Process\").getOrCreate()\n",
    "    data = sc.textFile(filename)\n",
    "#     sc.addPyFile(\"mfun.pyx\")\n",
    "    scores = sc.parallelize(data).map(mfun.word_count).collect()\n",
    "    sc.stop()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.randint(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \", \"<p> i understand payments are a tricky thing but i'm yet to find a worthy alternative to paypal. i want to change from paypal because i think they are expensive and it doesn't work in all countries. furthermore i think that the api is sufficient but could be better. the api documentation however is total utter <em> crap</em> .</p>   <p> i am looking for a payment / transaction service that is more developer friendly preferably with:</p>   <ul>  <li> <strong> a clean and well-structured rest api</strong> </li>  <li> <strong> excellent developer tools and a sandbox</strong> </li>  <li> <strong> good example api implementations</strong>  preferably in python or ruby</li>  <li> worldwide credit/debit card coverage</li>  <li> rates cheaper than paypal (or the possibility to choose a payment plan)</li>  </ul>   <p> i suppose google checkout is somewhat worthy but it requires both the developer and prospective purchasers to have a google account. any other suggestions are very much appreciated!</p> \", '\"<p> i am making a class to store a reference to an object if given an lvalue in the constructor and to store a copy if given an rvalue. this is fine and compiles:</p>   <pre> <code> template &lt;typename obj_t&gt; struct holder {     obj_t obj;      template &lt;typename obj_ref_t&gt;     holder(obj_ref_t &amp;&amp; o)         : obj(std::forward&lt;obj_ref_t&gt;(o))     {} };  template &lt;typename obj_t&gt; holder&lt;obj_t&gt; make_held(obj_t &amp;&amp; o) {     return holder&lt;obj_t&gt;(std::forward&lt;obj_t&gt;(o)); } </code> </pre>   <p> however when i add a destructor (even an empty one) to the class i get a compiler error:</p>   <pre> <code> invalid instantiation of non-const reference of type \"\"obj_t &amp;\"\" from an rvalue of type \"\"holder&lt;obj_t &amp;&gt;\"\" </code> </pre>   <p> why does adding a destructor somehow invoke a constructor call to create a wrapped object out of an already wrapped object?</p> \"', '<p> i am trying to compare performance of <code> pow(x2.0)</code>  and <code> pow(x2.0000001)</code>  and i though that <code> 2.0</code>  would be much faster but they are at the same speed. i even removed jit optimizations by running jar with <code> -xint</code>  parameter.</p>   <p> any idea why is that please?  thanks a lot!</p> ']\n"
     ]
    }
   ],
   "source": [
    "# print(d2[:10])\n",
    "# sc.stop()\n",
    "# d3 = d2[:100]\n",
    "# d3.to_csv('pq.csv')\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext(appName = 'TextProcess').getOrCreate()\n",
    "# def line(a):\n",
    "#     sp = a.split(\",\")\n",
    "#     return a.split(\"~\")\n",
    "# process('pq.csv')\n",
    "def word_count(row):\n",
    "    return len(row.split(\" \"))\n",
    "\n",
    "files = sc.textFile(\"pq_clean.csv\") \\\n",
    ".map(lambda line: line.split(\",\")[1]) \\\n",
    ".cache()\n",
    "\n",
    "import random\n",
    "sample = files.take(random.randint(0,10))\n",
    "print(sample)\n",
    "# .map(word_count) \\\n",
    "# .cache()\n",
    "# .sum()\n",
    "#     .map(lambda line: line.split(\",\")) \\\n",
    "#     .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(files)\n",
    "tf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bypass_serializer',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_is_pipelinable',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_jrdd_val',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_prev_jrdd',\n",
       " '_prev_jrdd_deserializer',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'func',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_barrier',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'preservesPartitioning',\n",
       " 'prev',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 5.14 ms, total: 27.9 ms\n",
      "Wall time: 3min 50s\n",
      "CPU times: user 5.95 ms, sys: 248 µs, total: 6.19 ms\n",
      "Wall time: 60.7 ms\n"
     ]
    }
   ],
   "source": [
    "# tf.take(1)\n",
    "%time idf = IDF().fit(tf)\n",
    "%time tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048576,[7357,27527,35920,40200,40649,105642,173606,186435,211440,214770,238153,252894,265159,274911,279832,292370,295235,296409,335453,372567,388504,433442,437751,469732,485029,493959,518279,584844,681187,702216,702740,734443,777769,784206,785264,840959,875351,877522,891534,897367,945367,968035],[3.8226116235622163,14.417285837939875,0.2262768335616402,0.02661803847734302,1.3978806171397122,0.48458119188022175,0.02610314563040646,0.9105784134421238,0.0133458282023745,2.939712438456726,0.009286160842474996,0.0,0.004363024528283841,0.7838649809652338,0.7153792751540569,1.2890116836652659,4.22106448003389,0.0016099999855304493,0.08281624648565256,0.2344022636881407,0.059679926569264526,1.0633573220846997,0.29617030641072484,0.5790244947824792,6.175301282732599,0.0,0.5560562241666805,8.122475324564101e-05,1.3219377634221123,0.0045081045813046925,0.07765881333741419,0.005384157928520484,0.13329220058346902,58.9201038318625,6.277094103388847,0.3649521630589995,0.0,4.752039326253392,0.043819907894643795,0.15768315668310295,0.0,0.008529106311069358])\n"
     ]
    }
   ],
   "source": [
    "# a = tfidf.collect()\n",
    "# print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at one file\n",
    "candidate = clean(d2['body'][4])\n",
    "# print(candidate)\n",
    "candidateTf = hashingTF.transform(candidate)\n",
    "candidateTfIdf = idf.transform(candidateTf)\n",
    "similarities = tfidf.map(lambda v: v.dot(candidateTfIdf) / (v.norm(2) * candidateTfIdf.norm(2)))\n",
    "for i in enumerate(similarities.collect()):\n",
    "    print(i)\n",
    "\n",
    "%time topFive = sorted(enumerate(similarities.collect()), key= lambda kv: kv[1])[0:5]\n",
    "# topFive = sorted(enumerate(similarities.collect()), key=lambda (k, v): -v)[0:5]\n",
    "for idx, val in topFive:\n",
    "    print(\"doc '%s' has score %.4f\" % (d2['body'][idx], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = similarities.collect()\n",
    "# print(min(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n"
     ]
    }
   ],
   "source": [
    "# print(max(a))\n",
    "# print(candidateTf.shape)\n",
    "print(hashingTF.numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;p&gt; i am trying to create a report to display ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt; i understand payments are a tricky thing b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;p&gt; i am making a class to store a reference t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt; i am trying to compare performance of &lt;cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt; when i am reading the code of &lt;a href=\"htt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               body\n",
       "0      0  <p> i am trying to create a report to display ...\n",
       "1      1  <p> i understand payments are a tricky thing b...\n",
       "2      2  <p> i am making a class to store a reference t...\n",
       "3      3  <p> i am trying to compare performance of <cod...\n",
       "4      4  <p> when i am reading the code of <a href=\"htt..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(\"pq_clean.csv\")\n",
    "# d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.29 ms, sys: 12 µs, total: 4.31 ms\n",
      "Wall time: 4.22 ms\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_representation = tfidf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_representation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "# tokenize = lambda doc: doc.lower().split(\" \")\n",
    "# sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True)#, tokenizer=tokenize)\n",
    "# %time sklearn_representation = sklearn_tfidf.fit_transform(d2['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashidf = HashingVectorizer()\n",
    "skh = hashidf.fit_transform(d2['body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344722, 1048576)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 37560)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/neal/miniconda3/envs/keras/lib/python3.7/site-packages/pyspark/serializers.py\", line 714, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(skh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(344722, 1517363)\n"
     ]
    }
   ],
   "source": [
    "# d2.head()\n",
    "# print(sklearn_tfidf.get_stop_words())\n",
    "# %time sklearn_tfidf.transform(d2['body'][:1])\n",
    "import numpy as np\n",
    "print(sklearn_representation.shape)\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    magnitude = np.linalg.norm(vector1)*np.linalg.norm(vector2)\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# print(np.sqrt(np.sum(candidate**2)))\n",
    "print(np.linalg.norm(candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() (1517363,)\n"
     ]
    }
   ],
   "source": [
    "candidate = sklearn_tfidf.transform(d2['body'][:1]).toarray()[0]\n",
    "target = sklearn_representation[3].toarray()\n",
    "print(candidate[0].shape, target[0].shape)\n",
    "# print()\n",
    "sim = []\n",
    "\n",
    "# a = np.dot(candidate[0], target[0])\n",
    "# %time a = cosine_similarity(candidate[0], target[0])\n",
    "# print(a)\n",
    "# print(a)\n",
    "\n",
    "# print(candidate.shape, sklearn_representation[3].shape)\n",
    "a = cosine_similarity(candidate, sklearn_representation[2].toarray()[0])\n",
    "# print(a)\n",
    "# print(a)\n",
    "# %time np.dot(candidate.T, sklearn_representation[2])\n",
    "# for count, i in enumerate(sklearn_representation[:1000]):\n",
    "#     sim.append([cosine_similarity(candidate, i.toarray()[0]), count])\n",
    "# # print(sim)\n",
    "# print(sorted(sim)[:5])\n",
    "\n",
    "def compare(candidate, n):\n",
    "    sim = []\n",
    "    for count, i in enumerate(sklearn_representation[:n]):\n",
    "        sim.append([cosine_similarity(candidate, i.toarray()[0]), count])\n",
    "    return sorted(sim, reverse=True)[:5]\n",
    "# print(len(df))\n",
    "# print(sklearn_representation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 1.56 s, total: 1min 27s\n",
      "Wall time: 44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0000000000000002, 0],\n",
       " [0.15263288080462353, 2233],\n",
       " [0.13922132245710933, 2108],\n",
       " [0.11908126026567464, 4772],\n",
       " [0.11846256923066482, 2416]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time compare(candidate, 10000) # 1.5s for 100, 12.8s for 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p> i am new in using sql with delphi. i have a table i.e.:</p>   <pre> <code> columnname1 columnname2 columnname3 value 1a    value2a     value3a value 1b    value2b     value3b value 1c    value2c     value3c </code> </pre>   <p> and so on. i have the table displayed on a <code> dbgrid</code>  so currently the adodataset is all connected ok. however i have a small loop</p>   <pre> <code> for i := 0 to adodataset.fieldcount - 1 do begin   writeln(wolfileadodataset.fields[i].asstring); end; </code> </pre>   <p> that writes to a text file. this text file does not have the column names and lists the fields one per line.</p>   <pre> <code> value 1a value 2a value 3a value 1b value 2b value 3b </code> </pre>   <p> etc. how can i get it to list in a row format similar to how i've represented the complete table above?</p> \n",
      " \n",
      "<p> i am trying to create a report to display a summary of the values of the columns for each row.   a basic analogy would an inventory listing.  say i have about 15 locations like 2a 2b 2c 3a 3b 3c etc.   each location has a variety of items and the items each have a specific set of common descriptions i.e. a rating of 1-9 boolean y or n another boolean y or n.  it looks something like this:</p>   <pre> <code> 2a   4       y       n 2a   5       y       y 2a   5       n       y 2a   6       n       n       ... 2b   4       n       y   2b   4       y       y       ...etc. </code> </pre>   <p> what i would like to produce is a list of locations and summary counts of each attribute:</p>   <pre> <code> location    1 2 3 4 5 6 7 8 9      y  n        y n      total 2a                1 2 1            2  2        2 2        4 2b                2                1  1        2          2 ... ___________________________________________________________ totals            3 2 1            3  3        4 2        6 </code> </pre>   <p> the query returns fields:  </p>   <pre> <code> location_cd string   desc_cd int  y_n_1 string  y_n_2 string </code> </pre>   <p> i have tried grouping by location but cannot get the summaries to work.   i tried putting it in a table but that would only take the original query.  i tried to create datasets for each unit and create variables in each one for each of the criteria but that hasn't worked yet either.  but maybe i am way off track and crosstabs would work better?  i tried that and got a total mess the first time.  maybe a bunch of subreports?</p>   <p> can someone point me in the correct direction please?    it seemed easy when i started out but now i am getting nowhere.  i can get the report to print out the raw data but all i need are totals for each column broken down out by location.  </p> \n"
     ]
    }
   ],
   "source": [
    "# print(d2['body'][2233])\n",
    "# print(\" \")\n",
    "# print(d2['body'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06933385 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# print(sklearn_representation[0])\n",
    "# print(candidate)\n",
    "# for i in candidate:\n",
    "# import scipy.sparse.csr_matrix\n",
    "# from scipy.sparse.csr_matrix import todense\n",
    "a = candidate.toarray()\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 3, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([0,2,3,6,4,8,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
